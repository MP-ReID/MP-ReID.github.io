<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Multi-modality and Multi-platform person re-identification benchmark and method.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Multi-modal Multi-platform Person Re-Identification: Benchmark and Method</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/file.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Multi-modal Multi-platform Person Re-Identification: Benchmark and Method</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a >Ruiyang Ha</a><sup>1</sup>,</span>
            <span class="author-block">
              <a >Songyi Jiang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a>Bin Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a>Bikang Pan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a>Yihang Zhu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.xjtlu.edu.cn/en/persons/JunjieZhang">Junjie Zhang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://xiatian-zhu.github.io/">Xiatian Zhu</a><sup>3</sup>
            </span>
            <span class="author-block">
              <a href="https://www.eecs.qmul.ac.uk/~sgg/">Shaogang Gong</a><sup>4</sup>
            </span>
            <span class="author-block">
              <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a><sup>1,*</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ShanghaiTech University,</span>
            <span class="author-block"><sup>2</sup>The Xi'an Jiaotong-Liverpool University</span>
            <span class="author-block"><sup>3</sup>University of Surrey</span>
            <span class="author-block"><sup>4</sup>Queen Mary University of London</span>
            <span class="author-block"><sup>*</sup>Corresponding authors</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2503.17096"*需要修改*
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.17096"*需要修改*
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Almost There)</span>
                </a>
              </span>

              <span class="link-block">
                <a href="">
                  <span class="icon">
                    <svg class="svg-inline--fa fa-database fa-w-14" aria-hidden="true" focusable="false"
                      data-prefix="fas" data-icon="database" role="img" xmlns="http://www.w3.org/2000/svg"
                      viewBox="0 0 448 512" data-fa-i2svg="">
                      <path fill="currentColor"
                        d="M448 73.143v45.714C448 159.143 347.667 192 224 192S0 159.143 0 118.857V73.143C0 32.857 100.333 0 224 0s224 32.857 224 73.143zM448 176v102.857C448 319.143 347.667 352 224 352S0 319.143 0 278.857V176c48.125 33.143 136.208 48.572 224 48.572S399.874 209.143 448 176zm0 160v102.857C448 479.143 347.667 512 224 512S0 479.143 0 438.857V336c48.125 33.143 136.208 48.572 224 48.572S399.874 369.143 448 336z">
                      </path>
                    </svg><!-- <i class="fas fa-database"></i> Font Awesome fontawesome.com -->
                  </span>
                  <span>Data (Almost There)</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Conventional person re-identification (ReID) research is often limited to single-
            modality sensor data from static cameras, which fails to address the complexities of
            real-world scenarios where multi-modal signals are increasingly prevalent. For
            instance, consider an urban ReID system integrating stationary RGB cameras, nighttime 
            infrared sensors, and UAVs equipped with dynamic tracking capabilities. Such systems 
            face significant challenges due to variations in camera perspectives, lighting
            conditions, and sensor modalities, hindering effective person ReID.
          </p>
          <p>
            To address these challenges, we introduce the <span class="dnerf">MP-ReID</span> benchmark, a novel dataset 
            designed specifically for multi-modality and multi-platform ReID. This benchmark
            uniquely compiles data from 1,930 identities across diverse modalities, including
            RGB, infrared, and thermal imaging, captured by both UAVs and ground-based cameras in 
            indoor and outdoor environments.            
          </p>
          <p>
            Building on this benchmark, we introduce Uni-Prompt ReID, a framework with specific-
            designed prompts, tailored for cross-modality and cross-platform scenarios. Our
            method consistently outperforms state-of-the-art approaches, establishing a robust
            foundation for future research in complex and dynamic ReID environments.
            Additionally, our dataset will be made publicly available to support further 
            advancements.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
<!--     <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
            <img src="./static/images/v1.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>A new MP-ReID dataset is represented by a conceptual diagram,
              showcasing the inclusion of six ground RGB cameras, six ground infrared cameras,
              one UAV RGB camera and one UAV thermal camera</p>
          </div>
      </div>
    </div>
  </div>
</section>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    

      <!-- Dataset. -->
      
        <div class="content">
          <h2 class="title is-3">MP-ReID Dataset</h2>
          <p>
            We established a new multi-modality and multi-platform dataset MP-ReID, which 
            consists 1,930 identities and 136,156 images sampled from over 1.2M frames.
            The images are collected from ground RGB and infrared cameras and UAV RGB
            and thermal cameras.
            <img src="./static/images/v3.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>MP-ReID dataset comprises three distinct modalities and three different scenes, with
              notable disparities between images captured in different modalities and scenes. Here
              showcase a range of variations to highlight the challenges present in person re-
              identification. From left to right, selected samples from different scenes and
              modalities illustrate the disparities between various viewpoints, instances of low
              resolution, cases of motion blur, and scenarios involving occlusion, respectively. These
              examples serve to demonstrate the complex nature of the gaps and obstacles within our 
              dataset, emphasizing the diversity and real-world applicability of the MP-ReID benchmark.
            </p>
          <p>
            Leveraging data collection from both ground-based cameras and UAV, our dataset contains
            video recordings capturing RGB, infrared, and thermal modalities across indoor, outdoor, and aerial domains.
            Noteworthy is the scale of our MP-ReID dataset, featuring 1,930 distinct identities and
            136,156 human bounding boxes acquired from 14 cameras. This scale is comparable to that
            of multi-view traditional datasets, thus establishing our dataset as a substantial
            resource for ReID research, featuring numerous cameras and human annotations across
            various platforms and modalities. To ensure robust privacy protection, we applied mosaic
            techniques to obscure pedestrians’ facial features in all video recordings and 
            permanently deleted the original raw footage to prevent any potential data leakage.
          </p>
          </p>
          
        </div>
      
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      
        <h2 class="title is-3">Uni-Prompt Method</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              To enhance cross-modality and cross-platform generalization in ReID, we introduce Uni-
              Prompt ReID, a unified prompt-based learning framework that leverages vision-language 
              alignment in VLMs like CLIP through multi-part text prompt fine-tuning.
            </p>
            <img src="./static/images/method.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>
              Building on DAPrompt, we incorporate modality and platform information into the text prompt.
              Additionally, inspired by CoCoOp, we enhance learnable prompts by designing a weight-tied
              meta-network to extract text prompt biases from pedestrian image visual features.
            </p>
            <p>
              Our framework employs a learnable text prompt structure, as illustrated in the figure,
              which integrates multi-modality and multi-platform information into prompt learning.
              The text prompt comprises three distinct components: (1) the Specific Re-ID Prompt,
              which encodes individual-specific information; (2) the Modality-aware Prompt, 
              which captures modality-specific details; and (3) the Platform-aware Prompt, 
              which incorporates platform-specific context. This design enhances the model’s 
              representational capacity by embedding rich, task-relevant information into the 
              prompt structure.
            </p>
          </div>

        </div>

    <h2 class="title is-3">Experiments</h2>
    <div class="columns is-centered">
          <div class="column content">
            <p>
              Following the existing works, we employed both Cumulated Matching Characteristic (CMC)
              at Rank-1 and Rank-5. To account for the retrieval of multiple instances and difficult 
              samples, we use mean average precision (mAP) as the accuracy metrics.
            </p>
            <img src="./static/images/exp.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>
              To validate the various characteristics of our proposed MP-ReID, we designed a total
              12 different experimental settings, considering all four image sources: ground RGB,
              ground infrared, UAV RGB and UAV thermal. It is important to note that, in order to
              address the complexity and variability of city-scale scenarios, it is essential to
              fully leverage the interaction between multiple platforms and the complementarity of
              different modalities.
            </p>
            <p>
              To this end, we propose four novel cross-modality and cross-platform ReID settings.
              The results from each experiment were recorded and subsequently categorized into 
              cross-modality,cross-platform, and cross-modality & platform. The mean performance of each method
              in each experimental category is shown in the experiment table.The findings in the 
              table reveal that most of the cross-modality methods perform well in cross-modality
              tasks but underperform in the cross modality & platform settings due to the lack of
              consideration for platform diversity. Our Uni-Prompt ReID method outperforms other
              state-of-the-art methods, with a 7.87% boost in average Rank-1 accuracy and a 15.42%
              higher mAP by on our MP-ReID dataset, demonstrating that Uni-Prompt ReID method exhibits
              strong robustness and high efficiency.
            </p>
          </div>

        </div>
    
    
    <!--/ Matting. -->

<!--     Concurrent Work. -->
<!--     <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      <div style="position: relative;">
          <pre><code id="bibtex-code">@article{ha2025multi,
            title={Multi-modal Multi-platform Person Re-Identification: Benchmark and Method},
             author={Ha, Ruiyang and Jiang, Songyi and Li, Bin and Pan, Bikang and Zhu, Yihang and Zhang, Junjie and Zhu, Xiatian and Gong, Shaogang and Wang, Jingya},
             journal={arXiv preprint arXiv:2503.17096},
            year={2025}
          }
<!--             整段待修改 -->
        </code></pre>
          <button onclick="copyToClipboard()" style="position: absolute; top: 5px; right: 5px; background: none; border: none; cursor: pointer;">
              <!-- 粘贴图标，可以使用Font Awesome 图标库 -->
              <i class="fa fa-paste" aria-hidden="true"></i>
          </button>
      </div>
  </div>
</section>


<footer class="footer">
    <div class="columns is-centered">
        <div class="content">
          <p>
            This website is borrowed from <a href="https://afforddp.github.io/">afforddp</a> and <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
    </div>
</footer>

</body>
</html>
